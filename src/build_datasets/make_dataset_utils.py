import os
import glob
import ipdb
import pickle
import json
import tiktoken


class CodexTokenizer:
    def __init__(self):
        self.tokenizer = tiktoken.get_encoding("p50k_base")

    def tokenize(self, text):
        return self.tokenizer.encode_ordinary(text)

    def decode(self, token_ids):
        return self.tokenizer.decode(token_ids)


class Tools:
    @staticmethod
    def read_code(fname):
        with open(fname, "r", encoding="utf8") as f:
            return f.read()

    @staticmethod
    def load_pickle(fname):
        with open(fname, "rb") as f:
            return pickle.load(f)

    @staticmethod
    def load_json(fname):
        with open(fname, "r", encoding="utf8") as f:
            return json.load(f)

    @staticmethod
    def dump_pickle(obj, fname):
        with open(fname, "wb") as f:
            pickle.dump(obj, f)

    @staticmethod
    def dump_jsonl(obj, fname):
        with open(fname, "w", encoding="utf8") as f:
            for item in obj:
                f.write(json.dumps(item) + "\n")

    @staticmethod
    def dump_json(obj, fname):
        with open(fname, "w", encoding="utf8") as f:
            json.dump(obj, f, indent=4)

    @staticmethod
    def load_jsonl(fname):
        with open(fname, "r", encoding="utf8") as f:
            lines = []
            for line in f:
                lines.append(json.loads(line))
            return lines

    @staticmethod
    def trim_context(tokenizer, previous_context_lines, context_max_tokens):
        previous_total_lines = len(previous_context_lines)
        previous_context = "\n".join(previous_context_lines)
        tokens = tokenizer.tokenize(previous_context)
        decoded_context_total_lines = tokenizer.decode(tokens).count("\n") + 1
        try:
            assert previous_total_lines == decoded_context_total_lines
        except AssertionError:
            ipdb.set_trace()
        trimmed_tokens = tokens[-context_max_tokens:]
        trimmed_context = tokenizer.decode(trimmed_tokens)
        trimed_context_total_lines = trimmed_context.count("\n") + 1
        trimed_context_start_lineno = (
            previous_total_lines - trimed_context_total_lines
        )  # 0-indexed lineno
        return trimmed_context.splitlines(), trimed_context_start_lineno

    @staticmethod
    def iterate_repository(repo):
        base_dir = "data/repositories"
        pattern = os.path.join(f"{base_dir}/{repo}", "**", "*.py")
        files = glob.glob(pattern, recursive=True)

        skipped_files = []
        loaded_code_files = dict()
        base_dir_list = os.path.normpath(base_dir).split(os.sep)
        for fname in files:
            try:
                code = Tools.read_code(fname)
                fpath_tuple = tuple(os.path.normpath(fname).split(os.sep)[len(base_dir_list) :])
                loaded_code_files[fpath_tuple] = code
            except Exception as e:
                skipped_files.append((fname, e))
                continue

        if len(skipped_files) > 0:
            print(f"Skipped {len(skipped_files)} out of {len(files)} files due to I/O errors")
            for fname, e in skipped_files:
                print(f"{fname}: {e}")
        return loaded_code_files

    @staticmethod
    def tokenize(code):
        tokenizer = CodexTokenizer()
        return tokenizer.tokenize(code)
