import os
import random

from make_dataset_utils import Tools, CodexTokenizer


class RandomHoleDigger:
    def __init__(
        self,
        repo_base_dir,
        repo,
        context_max_tokens=2000,
        line_min_tokens=5,
        max_sample_per_repo=200,
    ):
        self.source_code_files = Tools.iterate_repository(repo)
        self.context_max_tokens = context_max_tokens
        self.max_sample_per_repo = max_sample_per_repo
        self.line_min_tokens = line_min_tokens
        self.repo = repo
        self.tokenizer = CodexTokenizer()

    def _get_line_types(self, lines):
        line_types = dict()
        in_multiline_comment = False
        multiline_comment_start = ""
        for lineno, line in enumerate(lines):
            stripped_line = line.strip()
            if not stripped_line:
                line_types[lineno] = "empty"
                continue
            if in_multiline_comment:
                if stripped_line.endswith(multiline_comment_start):
                    in_multiline_comment = False
                line_types[lineno] = "comment"
            elif stripped_line.startswith('"""') or stripped_line.startswith("'''"):
                in_multiline_comment = True
                multiline_comment_start = stripped_line[:3]
                line_types[lineno] = "comment"
            elif stripped_line and stripped_line[0] == "#":
                line_types[lineno] = "comment"
            else:
                line_types[lineno] = "code"
        return line_types

    def _get_usable_lines(self, lines):
        line_types = self._get_line_types(lines)
        usable_lines = []
        for lineno, line_type in line_types.items():
            if line_type == "code":
                if lineno == 0:
                    continue
                if line_types[lineno - 1] == "empty":
                    continue
                usable_lines.append(lineno)
        return usable_lines

    def get_chosen_lines(self):
        candidate_lines = []
        for fpath_tuple, code in self.source_code_files.items():
            code_lines = code.splitlines()
            usable_lines = self._get_usable_lines(code_lines)
            candidate_lines.extend([(fpath_tuple, line_no) for line_no in usable_lines])
        random.shuffle(candidate_lines)
        chosen_lines = []
        chosen_line_strs = set()
        for fpath_tuple, line_no in candidate_lines:
            code_lines = self.source_code_files[fpath_tuple].splitlines()
            line = code_lines[line_no]
            if len(self.tokenizer.tokenize(line)) > self.context_max_tokens:
                continue
            if line.strip() in chosen_line_strs:
                continue
            chosen_line_strs.add(line.strip())
            chosen_lines.append(
                {
                    "fpath_tuple": fpath_tuple,
                    "line_no": line_no,
                    "ground_truth": line,
                    "code_lines": code_lines,
                }
            )
            if len(chosen_lines) >= self.max_sample_per_repo:
                break
        return chosen_lines

    def _make_context(self, line):
        previous_lines = line["code_lines"][: line["line_no"]]
        trimmed_lines, trimed_context_start_lineno = Tools.trim_context(
            self.tokenizer, previous_lines, self.context_max_tokens
        )
        return "\n".join(trimmed_lines), trimed_context_start_lineno

    def make_dataset(self):
        test_data = []
        chosen_lines = self.get_chosen_lines()
        for index, line in enumerate(chosen_lines):
            prompt, trimed_context_start_lineno = self._make_context(line)
            test_data.append(
                {
                    "prompt": prompt,
                    "metadata": {
                        "task_id": f"{self.repo}/{index}",
                        "ground_truth": line["ground_truth"],
                        "fpath_tuple": line["fpath_tuple"],
                        "context_start_lineno": trimed_context_start_lineno,
                        "line_no": line["line_no"],
                    },
                }
            )
        print(f"Generated {len(test_data)} samples for {self.repo}")
        return test_data


if __name__ == "__main__":
    OUT_BASE_DIR = "output"
    REPO_BASE_DIR = "downloaded_repos"
    repos = [
        "huggingface_diffusers",
        "nerfstudio-project_nerfstudio",
        "awslabs_fortuna",
        "huggingface_evaluate",
        "google_vizier",
        "PaddlePaddle_PaddleTS",
        "microsoft_RegionCLIP",
        "alibaba_FederatedScope",
        "pytorch_rl",
        "opendilab_ACE",
    ]
    lines = []
    for repo in repos:
        print(f"Processing {repo}")
        digger = RandomHoleDigger(REPO_BASE_DIR, repo)
        lines += digger.make_dataset()
    Tools.dump_jsonl(lines, os.path.join(OUT_BASE_DIR, "ten-repos-random-line-completion.jsonl"))
